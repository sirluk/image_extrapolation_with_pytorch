{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57fdcb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "from src.utils import get_borders\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from typing import Union  \n",
    "from torchvision import transforms\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9241463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAuto(nn.Conv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.padding =  (self.kernel_size[0] // 2, self.kernel_size[1] // 2) # dynamic add padding based on the kernel_size\n",
    "\n",
    "        \n",
    "class MultiConv(nn.Module):\n",
    "    def __init__(self, in_channels: int = 1, out_channels: int = 32, kernel_sizes: tuple = (7,), bias: bool = True):    \n",
    "        super(MultiConv, self).__init__()\n",
    "                \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.conv_dict = {}\n",
    "        \n",
    "        convs = []\n",
    "        for kernel_size in kernel_sizes:\n",
    "            convs.append(ConvAuto(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, bias=bias))\n",
    "        self.convs = torch.nn.Sequential(*convs)\n",
    "            \n",
    "        #1x1 convolution to map concatenated output of multiconv back to shape n_kernels\n",
    "        self.conv_1x1 = nn.Conv2d(in_channels=(out_channels * len(kernel_sizes)), out_channels=out_channels, kernel_size=1, bias=True)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x_ = torch.cat([c(x) for c in self.convs], dim=1)\n",
    "        return self.conv_1x1(x_)\n",
    "        \n",
    "\n",
    "class CNNBaseMulti(nn.Module):\n",
    "    def __init__(self, n_in_channels: int = 1, n_hidden_layers: int = 3, n_kernels: int = 32, kernel_sizes: Union[tuple, int] = (7,), batch_norm: bool = False, kernel_size_out: int = 7):\n",
    "        \"\"\"Simple CNN with `n_hidden_layers`, `n_kernels`, and `kernel_size` as hyperparameters\"\"\"\n",
    "        super(CNNBase, self).__init__()\n",
    "        \n",
    "        if type(kernel_sizes)==int:\n",
    "            kernel_sizes = tuple([kernel_sizes])\n",
    "        elif type(kernel_sizes)==list:\n",
    "            kernel_sizes = tuple(kernel_sizes)\n",
    "        \n",
    "        cnn = []\n",
    "        for i in range(n_hidden_layers):\n",
    "            cnn.append(MultiConv(in_channels=n_in_channels, out_channels=n_kernels, kernel_sizes=kernel_sizes, bias=True))\n",
    "            if batch_norm:\n",
    "                cnn.append(nn.BatchNorm2d(n_kernels))\n",
    "            cnn.append(torch.nn.ReLU())\n",
    "            n_in_channels = n_kernels\n",
    "        self.hidden_layers = torch.nn.Sequential(*cnn)\n",
    "\n",
    "        self.output_layer = torch.nn.Conv2d(in_channels=n_in_channels, out_channels=1,\n",
    "                                            kernel_size=kernel_size_out, bias=True, padding=int(kernel_size_out/2))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply CNN to input `x` of shape (N, n_channels, X, Y), where N=n_samples and X, Y are spatial dimensions\"\"\"\n",
    "        cnn_out = self.hidden_layers(x)  # apply hidden layers (N, n_in_channels, X, Y) -> (N, n_kernels, X, Y)\n",
    "        pred = self.output_layer(cnn_out)  # apply output layer (N, n_kernels, X, Y) -> (N, 1, X, Y)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9efc215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "811b9b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "tf = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Grayscale(),\n",
    "                transforms.Resize((90,90))\n",
    "])\n",
    "for f in Path(input_dir).rglob(\"*.jpg\"):\n",
    "    imgs.append(tf(Image.open(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7077306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96fa30d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2279, 0.2109, 0.1660,  ..., 0.2700, 0.2864, 0.1379],\n",
       "         [0.2030, 0.1680, 0.1920,  ..., 0.0698, 0.2075, 0.4795],\n",
       "         [0.2294, 0.1424, 0.1086,  ..., 0.3743, 0.3833, 0.4518],\n",
       "         ...,\n",
       "         [0.3333, 0.2233, 0.3995,  ..., 0.4441, 0.2631, 0.1029],\n",
       "         [0.3925, 0.3777, 0.2783,  ..., 0.4684, 0.2546, 0.1677],\n",
       "         [0.3905, 0.3388, 0.3239,  ..., 0.4263, 0.2075, 0.1802]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad2d0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = MultiConv(in_channels = 1, out_channels = 32, kernel_sizes = (7,9,11), bias = True)\n",
    "conv2 = MultiConv(in_channels = 32, out_channels = 32, kernel_sizes = (5,7,9), bias = True)\n",
    "conv3 = MultiConv(in_channels = 32, out_channels = 32, kernel_sizes = (5,7,9), bias = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6ddad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = img.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0139135",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 1\n",
    "out_channels = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6a90e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_channels: int = 1, out_channels: int = 32, kernel_sizes: tuple = (5,7), bias: bool = True):    \n",
    "        super(MultiConv, self).__init__()\n",
    "                \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.conv_dict = {}\n",
    "        \n",
    "        convs = []\n",
    "        for kernel_size in kernel_sizes:\n",
    "            convs.append(ConvAuto(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, bias=bias))\n",
    "        self.convs = torch.nn.Sequential(*convs)\n",
    "            \n",
    "        #1x1 convolution to map concatenated output of multiconv back to shape n_kernels\n",
    "        self.conv_1x1 = nn.Conv2d(in_channels=(out_channels * len(kernel_sizes)), out_channels=out_channels, kernel_size=1, bias=True)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x_ = torch.cat([c(x) for c in self.convs], dim=1)\n",
    "        return self.conv_1x1(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9cb670c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_blocks = nn.Sequential(\n",
    "    conv_layer(1), #conv_1x1\n",
    "    nn.Sequential(\n",
    "        ConvAuto(in_channels=in_channels, out_channels=out_channels, kernel_size=1, bias=True),\n",
    "        ConvAuto(in_channels=out_channels, out_channels=out_channels, kernel_size=3, bias=True)\n",
    "    ), #conv_3x3\n",
    "    nn.Sequential(\n",
    "        ConvAuto(in_channels=in_channels, out_channels=out_channels, kernel_size=1, bias=True),\n",
    "        ConvAuto(in_channels=out_channels, out_channels=out_channels, kernel_size=5, bias=True)\n",
    "    ), #conv_5x5\n",
    "    nn.Sequential(\n",
    "        nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "        ConvAuto(in_channels=in_channels, out_channels=out_channels, kernel_size=1, bias=True)\n",
    "    ) #max_pool_3x3   \n",
    ")\n",
    "\n",
    "filter_concat = ConvAuto(in_channels=out_channels * 4, out_channels=out_channels, kernel_size=1, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e4f61e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = torch.cat([c(x) for c in conv_blocks], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b557bb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 90, 90])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "462f9b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 90, 90])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_concat(x_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ec19616a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 90, 90])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pool_3x3(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4fbb8cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ffa85166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 90, 90])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "22eb2d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ConvAuto(1, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (1): Sequential(\n",
       "    (0): ConvAuto(1, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ConvAuto(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): ConvAuto(1, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ConvAuto(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "    (1): ConvAuto(1, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a87af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    tf_aug = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.RandomResizedCrop((90,90)),\n",
    "                transforms.RandomHorizontalFlip(.33),\n",
    "                transforms.RandomVerticalFlip(.33)\n",
    "                #transforms.Normalize((mean,), (std,))\n",
    "            ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d6edae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_list = [\n",
    "    transforms.Lambda(lambda x: x)\n",
    "    transforms.Lambda(lambda x: torch.rot90(x, 1, dims=(2,3))),\n",
    "    transforms.Lambda(lambda x: torch.rot90(x, 2, dims=(2,3))),\n",
    "    transforms.Lambda(lambda x: torch.rot90(x, 3, dims=(2,3))),\n",
    "    transforms.Lambda(lambda x: torch.flip(x, dims=(2,))),\n",
    "    transforms.Lambda(lambda x: torch.flip(x, dims=(3,)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "25414bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = transforms.RandomHorizontalFlip(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9d6ff1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randint(0, len(tf_list), size=(1,)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "441f8502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotation_flip(x):\n",
    "    \"\"\"only works for square tensors\"\"\"\n",
    "    tf_list = [\n",
    "        lambda x: x,\n",
    "        lambda x: torch.rot90(x, 1, dims=(2,3)),\n",
    "        lambda x: torch.rot90(x, 2, dims=(2,3)),\n",
    "        lambda x: torch.rot90(x, 3, dims=(2,3)),\n",
    "        lambda x: torch.flip(x, dims=(2,)),\n",
    "        lambda x: torch.flip(x, dims=(3,))\n",
    "    ]\n",
    "    idx = torch.randint(0, len(tf_list), size=(1,)).item()\n",
    "    return tf_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "428a09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = transforms.Lambda(random_rotation_flip(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e3a25445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.3905, 0.3925, 0.3333,  ..., 0.2294, 0.2030, 0.2279],\n",
       "          [0.3388, 0.3777, 0.2233,  ..., 0.1424, 0.1680, 0.2109],\n",
       "          [0.3239, 0.2783, 0.3995,  ..., 0.1086, 0.1920, 0.1660],\n",
       "          ...,\n",
       "          [0.4263, 0.4684, 0.4441,  ..., 0.3743, 0.0698, 0.2700],\n",
       "          [0.2075, 0.2546, 0.2631,  ..., 0.3833, 0.2075, 0.2864],\n",
       "          [0.1802, 0.1677, 0.1029,  ..., 0.4518, 0.4795, 0.1379]]]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "49e7357d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2279, 0.2109, 0.1660,  ..., 0.2700, 0.2864, 0.1379],\n",
       "          [0.2030, 0.1680, 0.1920,  ..., 0.0698, 0.2075, 0.4795],\n",
       "          [0.2294, 0.1424, 0.1086,  ..., 0.3743, 0.3833, 0.4518],\n",
       "          ...,\n",
       "          [0.3333, 0.2233, 0.3995,  ..., 0.4441, 0.2631, 0.1029],\n",
       "          [0.3925, 0.3777, 0.2783,  ..., 0.4684, 0.2546, 0.1677],\n",
       "          [0.3905, 0.3388, 0.3239,  ..., 0.4263, 0.2075, 0.1802]]]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1429d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
